{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "headed-guest",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "### Jennefer Maldonado\n",
    "\n",
    "This jupyter notebook contains the fourth order Runge Kutta method, the fourth-rorder Adams-Bashforth Predictor, and Adams-Moulton Corrector. The functions will be tested on the ODE system modeling chemical reaction kinetics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "smoking-conspiracy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runge kutta approx for k2 = 10, y'= [ 2.28767925e+13  3.55330217e+78 -3.55330217e+78]\n",
      "adams approx for k2 = 10, y' = [-3.00000000e+00 -5.71851556e+10  5.71851556e+10]\n",
      "BDF approx for k2 = 10, y' = [   2.          318.36363636 -315.45454545]\n",
      "runge kutta approx for k2 = 100, y'= [ 2.28767925e+013  2.61871612e+193 -2.61871612e+193]\n",
      "adams approx for k2 = 100, y' = [-3.0000000e+00 -1.5903126e+27  1.5903126e+27]\n",
      "BDF approx for k2 = 100, y' = [ 2.00000000e+00  4.36898382e+06 -4.36898091e+06]\n",
      "runge kutta approx for k2 = 1000, y'= [ 2.28767925e+013  5.57253614e+305 -5.57253614e+305]\n",
      "adams approx for k2 = 1000, y' = [-3.00000000e+00 -1.78511729e+43  1.78511729e+43]\n",
      "BDF approx for k2 = 1000, y' = [ 2.00000000e+00  4.52732716e+10 -4.52732716e+10]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# matrix of ODE values\n",
    "def f(k2, y):\n",
    "    A = np.zeros((3,3))\n",
    "    A[0][0] = 1\n",
    "    A[1][0] = 1\n",
    "    A[1][1] = -k2\n",
    "    A[2][1] = k2\n",
    "    Ay = np.matmul(A, np.transpose(y))\n",
    "    return Ay\n",
    "\n",
    "#Runge-Kutta\n",
    "def runge_kutta(h, k2, y):\n",
    "    a1 = f( k2, y)\n",
    "    a2 = f( k2, np.add(y,a1) )\n",
    "    a3 = f( k2, np.add(y,a2*(h/2)) )\n",
    "    a4 = f( k2, np.add(y, h*a3))\n",
    "    A1 = np.add(a1, 2*a2)\n",
    "    A2 = np.add(2*a3, a4)\n",
    "    return np.add(y, (h/6)*(np.add(A1, A2)))\n",
    "\n",
    "# Fourth Order Adams-Bashforth Predictor\n",
    "def adams(h, y, y_p, y_p1, y_p2, y_p3):\n",
    "    y_a1 = np.add(55*y_p, -59*y_p1)\n",
    "    y_a2 = np.add(37*y_p2, -9*y_p3)\n",
    "    y_a = np.add(y_a1, y_a2)\n",
    "    return np.add(y, (h/24)*y_a)\n",
    "\n",
    "#Backward Differentiation Formulas\n",
    "def BDF(h, yk, y_km1, y_km2, y_p):\n",
    "    y1 = np.add(18*yk, -9*y_km1)\n",
    "    y1 = np.add(yk, y_km1)\n",
    "    y2 = np.add(y1, 2*y_km2)\n",
    "    y_kp1 = np.add((1/11)*y2, (6*h/11)*y_p)\n",
    "    return y_kp1 \n",
    "\n",
    "# initial conditions\n",
    "y = np.array([1,1,1])\n",
    "# step size\n",
    "h = 1\n",
    "# k2 value\n",
    "K2 = [10, 100, 1000]\n",
    "# last value that works without overflow\n",
    "tmax = 28\n",
    "for k2 in K2:\n",
    "    t = 0\n",
    "    yk = y\n",
    "    y_primes = []\n",
    "    while t < tmax:\n",
    "        y_new = runge_kutta(h, k2, yk)\n",
    "        if t < 4:\n",
    "            y_primes.append(y_new)\n",
    "        yk = y_new\n",
    "        t+=1\n",
    "\n",
    "    print('runge kutta approx for k2 = ' + str(k2) + \", y'= \" + str(yk))\n",
    "    a = adams(h, y, y_primes[0], y_primes[1], y_primes[2], y_primes[3])\n",
    "    print('adams approx for k2 = ' + str(k2)+ \", y' = \" + str(a))\n",
    "    b = BDF(h, y, y, y, y_primes[0])\n",
    "    print('BDF approx for k2 = ' + str(k2) + \", y' = \" + str(b)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-madrid",
   "metadata": {},
   "source": [
    "## Analysis and Comparison\n",
    "Runge Kutta performs the best out of the methods tested here. It does not require prior time steps to proceed, it self starting and it is easy to change the step size during integration. The Adams-Bashforth Predictor is another popular multistep method, which unlike runge kutta, it needs previous derivatives to compute. Implicit methods are typically more stable and more accurate than the corresponding explicit method of same order. In this case Runge Kutta out performs the Adams method. The backward differentiation formulas are implicit formulas as well and are effective for stiff ODEs. There is also still a chance that even though an implicit method is more stable than an explicit method it will not be unconditionally stable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-basin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
